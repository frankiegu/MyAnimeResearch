COCO-GAN这样的坐标支持模型，对动漫这样尺寸不规则的数据集可能非常有效

对于插画上色问题，可以通过操纵隐空间来上色，解耦模型（需实验），训练集为线稿与插画，无需对应，训练策略是，鉴别真实度，其他什么都不干；之后的上色就是将插画投影到隐空间，操作隐空间来重上色，也可输入线稿优化后来上色，也可输入插画来抽取线稿

动漫图像的low-level（低层级）特征；TODO

!3D优于2D图像处理说!neural rendering这种转成3D处理的问题；而是否很多地方（如动画中割，上色，如光照处理，等等），如果用3D，是否一个问题全都能解决？

(2D手绘中割，神经光场不靠谱，没有用律表做数据集；实验证明 神经表征过于稀疏，风格样本太少，3D才能做这样问题)

对于动画上色作业，是否可以取代，训练一个网络让其从零（草稿原画）开始上色，而是从前后两个已上色的分镜做参照，训练一个短时记忆网络来给中割的动画草稿上色？

关于动漫scenecut；图片流一帧帧处理，那确实难 但如果用字幕，利用duration与文字内容，上去预测场景切割位置 然后用时间点反过来标注视频

关于动漫scenecut；可以加速减速视频用于增广

如今的低质量（贫穷）摄影，色彩设计对动画观感影响颇大，本来内容还行的动画，用了这种新技法，瞬间低了一个档次让观众没兴趣看，提出anime2anime

可以避免冗余标注，保留片段的关键帧，人工标注少量，实现短时中割模型 之后并不需要每帧都标（不仅工作量大而且会引入大量的偏置），直接将短时中割模型整合得到 时序特征相关的分割模型 再将其整合进语言模型（词模型，上下文），就能生成有意义的分镜片段描述
