动漫图像的low-level（低层级）特征；TODO

对于动画上色作业，是否可以取代，训练一个网络让其从零（草稿原画）开始上色，而是从前后两个已上色的分镜做参照，训练一个短时记忆网络来给中割的动画草稿上色？

关于动漫视频摘要；图片流一帧帧处理，那确实难 但如果用字幕，利用duration与文字内容，上去预测场景切割位置 然后用时间点反过来标注视频

如今的低质量（贫穷）摄影，色彩设计对动画观感影响颇大，本来内容还行的动画，用了这种新技法，瞬间低了一个档次让观众没兴趣看，提出anime2anime

可以避免冗余标注，保留片段的关键帧，人工标注少量，实现短时中割模型 之后并不需要每帧都标（不仅工作量大而且会引入大量的偏置），直接将短时中割模型整合得到 时序特征相关的分割模型 再将其整合进语言模型（词模型，上下文），就能生成有意义的分镜片段描述
