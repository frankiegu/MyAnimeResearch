格式混乱，请用raw模式打开此文件

工程目标类似DAVIS Challenge，summe，tvsum，THUMOS
https://davischallenge.org
http://cvlab.hanyang.ac.kr/coview2019/
https://www.crcv.ucf.edu/THUMOS14/download.html

[gwern works (danbooru2019 etc.)](https://www.gwern.net/Crops#danbooru2019-portraits)

充分利用资源与设计数据集，部分思想：

不同于通用视频目标跟踪，动作识别等（video_labeler，vatic，UltimateLabeling）
固定镜头的采集数据方法，并保留了时间上的连续性等特征

动画的时间元素则是片段（例如sakugabooru），切分这些基本元素交给有复杂语义的标注系统。这是基本思路
然后用于 视频摘要，视频理解，视频压缩，视频风格迁移 等任务
> video summarization, video understanding,anime like video compression, video style transfer
例如danbooru众包的方式来标注，制作出真。动画数据集
制作各种video related任务，提出的SOTA模型上跑compare，benchmark之类的

### 提供众包收集database
（片源自己下，不需要像sakugabooru一样带视频，用提供的标注系统 来标注与提交贡献）
（像是基于git就能实现）github开不同文件夹，分task，动画全名（索引 等meta data）
> 元信息：动画公司，导演，分镜演出作监~（参考此仓库的link.md）
提供pairs文件，但不提供动画文件（提供bt，完整制作需 手动下载bd，或提供时轴校正补丁）

### 在线提交系统
对画质，有无字幕的要求，并不高，实际上，大多数任务都能做
只要不是修正或剪辑，tv版 带提供（时轴校正补丁可以考虑）
就可以开发，未来考虑 对接版权流，视频流的等在线标注插件

#### 最基本的，lowlevel
视频摘要，片段，或很明显的镜头切换（关键帧，角色动作）【短时中割/短时自动动画，短时（参照）上色模型，动画动态视频压缩】
可以参照aegisub keyframe Timing 实现
> 我认为：可以避免冗余标注，保留片段的关键帧，人工标注少量，实现短时中割模型
> 之后并不需要每帧都标（不仅工作量大而且会引入大量的偏置），直接将短时中割模型整合得到 时序特征相关的分割模型
> 再将其整合进语言模型（词模型，上下文），就能生成有意义的分镜片段描述

#### 高级任务 advanced task
从摘要片段抽取关键帧，可以做实例分割，多标签类别（场景，角色上下文相关等等）【静态图像任务，提供短时中割的实验集】
视频理解：分镜是片段集合，分镜语义（摄影技法，分镜语义，角色行为等等）

**保留这些，将使此数据集可做的任务十分多样化**
音频理解：补丁包，提供音频，字幕理解数据集（效果音，角色对话，背景音乐）
语言理解：仅轻小说改编，未来考虑整合

未来还可以考虑【**多模态理解，参考deepmind的[《Look, Listen and Learn》](https://arxiv.org/abs/1705.08168)&[《Objects that Sound》](https://arxiv.org/abs/1712.06651)**】

**不仅限以上任务，提供这样的一个task related数据库，将对动画在深度学习的研究产生重大影响**

#### 反正也没啥版权/法律问题，但我还是附一下
> 关于收集动画，插画，漫画，等等进行机器学习研究的法律，版权问题？
> 除生成模型以外，无任何问题，（不限于“非商业用途”），收集动漫数据对第三方进行培训并发布经过培训的模型是安全的。

> 从第三方的原始数据创建数据库，通过对数据库进行标签处理等来创建学习数据集并提供和出售数据集的行为 - OK（分类模型，检测模型等）
> 用第三方的学习数据集执行机器学习以生成训练模型并提供和出售训练模型的行为 - NG

> 日本改正著作権法「著作権法４７条の７」
> 進化する機械学習パラダイス ～改正著作権法が日本のAI開発をさらに加速する～
> 機械学習に使うデータセットの著作権について
